---
title: "IDS 572 - Decision Trees(2), Classifer performance evaluation"
author: "sidb"
date: "Sept 10, 2019"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


####We will continue with decision tree models from the earlier R exercise
```{r}
#Install the 'rpart' package to develop deciction trees
library('rpart')

```

###Load the data, and clean
```{r results='hide'}
#read the data, and examine summary statistics
mdData=read.csv('D:/ids572/class/MortgageDefaultersDataSample.csv')
summary(mdData)

#cleanup -- remove the Status variable (from which the dependent variable, Outcome, is determined).
#  Also remove the State variable (assume we do not want to use it here)
md<- within(mdData, rm(State, Status))
 # or you can remove by column number
 #    md <- mdData[, -c(11,13)]

#make sure that the variabes are set to the correct attribute type -- factor, integer, numeric
str(md)

#Inspect the data -- LoanValuetoAppraised should not be a Factor(ie. categorical) variable.
#Check the values -- looks like there are some values of "#DIV/0!"
#Change these to 0.0

md$LoanValuetoAppraised<-as.numeric(gsub("#DIV/0", "0.0", md$LoanValuetoAppraised))

#replace NA values with 0.0  (may not be a good thing to do!!)
md[is.na(md)]<-0.0

#Inspect the summary on column values
summary(md)
```

###Split data into training, validation sets, 
```{r}
#split the data into training and test(validation) sets - 70% for training, rest for validation
TRG_PCT=0.7
nr=nrow(md)
trnIndex = sample(1:nr, size = round(TRG_PCT*nr), replace=FALSE) #get a random 70%sample of row-indices
mdTrn=md[trnIndex,]   #training data with the randomly selected row-indices
mdTst = md[-trnIndex,]  #test data with the other row-indices
```

###Develop a model, plot the tree
```{r}
#develop a tree on the training data
rpModel1=rpart(OUTCOME ~ ., data=mdTrn, method="class")

#plot the tree
library(rpart.plot)
rpart.plot::prp(rpModel1, type=2, extra=1)
```

###Examine the DT model 
```{r results='hide'}
summary(rpModel1)

#Interpret the CP values and potential pruning, the variable implortance,...
#Do you see the surrogate variables at each node
```


###Best pruning level
```{r}
#The default parameters for rpart (see https://www.rdocumentation.org/packages/rpart/versions/4.1-13/topics/rpart.control)
#use cp=0.01 -- this prevent splits which do not improve fit by the specified value (0.01)

#We can instead grow a full tree and then prune based on the CP value which gives lowest cross-validated errors.

#Grow a tree, with cp=0
rpModel2 = rpart(OUTCOME ~ ., data=mdTrn, method="class", control = rpart.control(cp = 0.0))
#Is this a larger tree?

#check the errors for different levels of CP
printcp(rpModel2)
#plot the errors for different CP levels (https://www.rdocumentation.org/packages/rpart/versions/4.1-13/topics/plotcp)
plotcp(rpModel2)

#The best pruned tree can be considered as simplest model with error within 1 standard error of the minimum error #tree. The line in the plot above shows the 1 std err above the minimum error. So, which cp level would you use for #the best pruned tree?

#we can then prune the tree using the best cp value -- for example
rpModel2_pruned = prune(rpModel2, cp=0.0015)
```


##Examine model performance
```{r}
#Obtain the model's predictions on the training data
predTrn=predict(rpModel1, mdTrn, type='class')
#Confusion table
table(pred = predTrn, true=mdTrn$OUTCOME)
#Accuracy
mean(predTrn==mdTrn$OUTCOME)

#Or you can combine the above two steps as:
# table(pred=predict(rpModel1,mdTrn, type="class"), true=mdTrn$OUTCOME)
#to get the prob for the two classes, use predict(...without the type='class')

#Obtain the model's predictions on the test data
 #combining the two steps above
table(pred=predict(rpModel1, mdTst, type="class"), true=mdTst$OUTCOME)
mean(predict(rpModel1, mdTst, type="class")==mdTst$OUTCOME)


#Comparewith the second model we developed (rpModel2_pruned)
table(pred=predict(rpModel2_pruned, mdTst, type="class"), true=mdTst$OUTCOME)

# Question - was the first model pre-maturely over pruned?
```


###Apply different classification thresholds and examine performance
```{r}
CTHRESH=0.5

predProbTrn=predict(rpModel1, mdTrn, type='prob')
#Confusion table
predTrn = ifelse(predProbTrn[, 'default'] >= CTHRESH, 'default', 'non-default')
ct = table( pred = predTrn, true=mdTrn$OUTCOME)
#Accuracy
mean(predTrn==mdTrn$OUTCOME)
```


###Lift curve
```{r}
#get the 'scores' from applying the model to the data
predTrnProb=predict(rpModel1, mdTrn, type='prob')
head(predTrnProb)
```
So the firts column in predTrnProb give the predicted prob(default) -- assume 'default' is the class of interest. Next we sort the data based on these values, group into say, 10 groups (deciles), and calculate cumulative response in each group
```{r}
#we need the score and actual class (OUTCOME) values
trnSc <- subset(mdTrn, select=c("OUTCOME"))  # selects the OUTCOME column into trnSc
trnSc["score"]<-predTrnProb[, 1]  #add a column named 'Score' with prob(default) values in teh first column of predTrnProb

#sort by score
trnSc<-trnSc[order(trnSc$score, decreasing=TRUE),]

#we will next generate the cumulaive sum of the OUTCOME values -- for this, we want 'default'=1, and 'non-default'=0.   Notice that the OUTCOME is a factor (ie. categorical) variable
str(trnSc)
```
```{r}
levels(trnSc$OUTCOME)
```
So we should convert these to appropriate integer values 1 and 0
```{r}
levels(trnSc$OUTCOME)[1]<-1
levels(trnSc$OUTCOME)[2]<-0
# this has not changed OUTCOME -- it will now have factor levels '1' and '0'
trnSc$OUTCOME<-as.numeric(as.character(trnSc$OUTCOME))
str(trnSc)
```
```{r} 
#obtain the cumulative sum of default cases captured
trnSc$cumDefault<-cumsum(trnSc$OUTCOME)
head(trnSc)
```
```{r}
#Plot the cumDefault values (y-axis) by numCases (x-axis)
plot(seq(nrow(trnSc)), trnSc$cumDefault,type = "l", xlab='#cases', ylab='#default')
```



###ROC curves (using the ROCR package)
```{r}
library('ROCR')

#obtain the scores from the model for the class of interest, here, the prob('default')
scoreTst=predict(rpModel1,mdTst, type="prob")[,'default']  
   #same as predProbTst

#now apply the prediction function from ROCR to get a prediction object
rocPredTst = prediction(scoreTst, mdTst$OUTCOME, label.ordering = c('non-default', 'default'))  

#obtain performance using the function from ROCR, then plot
perfROCTst=performance(rocPredTst, "tpr", "fpr")
plot(perfROCTst)
```


###Optimal cutoff from ROC
```{r}
costPerf = performance(rocPredTst, "cost")
rocPredTst@cutoffs[[1]][which.min(costPerf@y.values[[1]])]
```

###optimal cost with different costs for fp and fn
```{r}
costPerf = performance(rocPredTst, "cost", cost.fp = 2, cost.fn = 1)
rocPredTst@cutoffs[[1]][which.min(costPerf@y.values[[1]])]
```

###other performance measures with the performance function
```{r}
accPerf = performance(rocPredTst, measure = "acc")
plot(accPerf)

#AUC vlaue
aucPerf = performance(rocPredTst, measure = "auc")
aucPerf@y.values
```

####Examine the ROCR prediction object
```{r}
class(rocPredTst)
slotNames(rocPredTst)
sapply(slotNames(rocPredTst), function(x) class(slot(rocPredTst, x)))
sapply(slotNames(rocPredTst), function(x) length(slot(rocPredTst, x)))
```

The prediction object has a set of slots with names as seen above.
The slots hold values of predictions,  labels, cutoffs, fp values, ....etc.

####Similarly, examine the ROCR performance object
```{r}
class(accPerf)
slotNames(accPerf)

accPerf@x.name   #values in the x.name slot
accPerf@y.name
```
So the x.values give the cutoff values and the y.values goves the corresponding accuracy. 
We can use these to find, for example, the cutoff corresponding to the maximum accuracy

####Optimal cutoff
```{r}
#The accuracy values are in y.values slot of the acc.perf object. 
#This is a list, as seen by: 
class(accPerf@y.values)
#... and we can get the values through 
accPerf@y.values[[1]]

#get the index of the max value of accuracy
ind=which.max(accPerf@y.values[[1]])

#get the accuracy value coresponding to this index
acc = (accPerf@y.values[[1]])[ind]

#get the cutoff corresponding to this index
cutoff = (accPerf@x.values[[1]])[ind]

#show these results
print(c(accuracy= acc, cutoff = cutoff))
```

Similarly, for the optimal ROC curve based cutoff (using the "cost" as performance measure), we can get the corresponding TP and FP values
```{r}
costPerf = performance(rocPredTst, "cost")
optCutoff = rocPredTst@cutoffs[[1]][which.min(costPerf@y.values[[1]])]
optCutInd = which.max(perfROCTst@alpha.values[[1]] == optCutoff)

#get the ROC curve values, i.e. TPRate and FPRate for different cutoffs
perfROCTst=performance(rocPredTst, "tpr", "fpr")
#You should check the slotNames of perROCTst....the FPRate is in x.values and TPRate is in y.values

OptCutoff_FPRate = perfROCTst@x.values[[1]][optCutInd]
OptCutoff_TPRate = perfROCTst@y.values[[1]][optCutInd]
print(c(OptimalCutoff=optCutoff, FPRAte=OptCutoff_FPRate, TPRate =OptCutoff_TPRate))

```

###Calculate the 'profit' lift for a model
- for this we need the scores given by a model, and the actual class values.
Assume a 'profit' value for correctly predicting a 'default' case, and a 'cost' for mistakes.
First, sort by descending score values
Then calculate the profits, and then the cumulative profits
```{r}
library(dplyr)

PROFITVAL=3
COSTVAL=-2

scoreTst=predict(rpModel1,mdTst, type="prob")[,'default'] 
prLifts=data.frame(scoreTst)
prLifts=cbind(prLifts, mdTst$OUTCOME)
     #check what is in prLifts ....head(prLifts)

prLifts=prLifts[order(-scoreTst) ,]  #sort by descending score

#add profit and cumulative profits columns
prLifts$profits <- ifelse(prLifts$`mdTst$OUTCOME`=='default', PROFITVAL, COSTVAL)
prLifts$cumProfits <- cumsum(prLifts$profits)

plot(prLifts$cumProfits)

#find the score coresponding to the max profit
maxProfit= max(prLifts$cumProfits)
maxProfit_Ind = which.max(prLifts$cumProfits)
maxProfit_score = prLifts$scoreTst[maxProfit_Ind]
print(c(maxProfit = maxProfit, scoreTst = maxProfit_score))
```


###Random forest models 
  - install the 'randomForest' library first
```{r}
library('randomForest')

#for reproducible results, set a specific value for the random number seed
set.seed(123)

#develop a model with 200 trees, and obtain variable importance
rfModel = randomForest(OUTCOME ~ ., data=mdTrn, ntree=200, importance=TRUE )
#check the model -- see what OOB error rate it gives

#Variable importance
importance(rfModel)
varImpPlot(rfModel)

#Draw the ROC curve for the randomForest model
perfROC_rfTst=performance(prediction(predict(rfModel,mdTst, type="prob")[,2], mdTst$OUTCOME), "tpr", "fpr")
plot(perfROC_rfTst)

#Draw the lift curve fr teh random forest model
perfLift_rfTst=performance(prediction(predict(rfModel,mdTst, type="prob")[,2], mdTst$OUTCOME), "lift", "rpp")
plot(perfLift_rfTst)

```


####Multiple ROC curves on same plot
```{r}
#ROC curves for the decision-tree model and the random forest model in the same plot 

perfROC_dt1Tst=performance(prediction(predict(rpModel1,mdTst, type="prob")[,2], mdTst$OUTCOME), "tpr", "fpr")
perfRoc_dt2Tst=performance(prediction(predict(rpModel2_pruned,mdTst, type="prob")[,2], mdTst$OUTCOME), "tpr", "fpr")
perfRoc_rfTst=performance(prediction(predict(rfModel,mdTst, type="prob")[,2], mdTst$OUTCOME), "tpr", "fpr")

plot(perfROC_dt1Tst, col='red')
plot(perfRoc_dt2Tst, col='blue', add=TRUE)
plot(perfRoc_rfTst, col='green', add=TRUE)
legend('bottomright', c('DecisionTree-1', 'DecisionTree-2', 'RandomForest'), lty=1, col=c('red', 'blue', 'green'))

```



